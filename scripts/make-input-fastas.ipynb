{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README: Making input fasta files\n",
    "\n",
    "This script wrangles multiple sequence alignments and fasta files from `Nextstrain/fauna` to make fasta files of sequences that we want to include specifically in our analysis of Zika in the Americas. A variety of fasta files are outputted, including ones that can be read in to BEAST, and ones that can be read in to `Nextstrain/augur`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### import libraries ####\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "import datetime\n",
    "\n",
    "date = datetime.datetime.now().strftime (\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### infile paths #### \n",
    "zika_msa_stripped = \"/Users/alliblk/Desktop/gitrepos/augur/zika/processed/zika_aligned_stripped.mfa\"\n",
    "fauna_file = \"/Users/alliblk/Desktop/gitrepos/fauna/data/zika.fasta\"\n",
    "\n",
    "#### outfile paths #### \n",
    "americas_file = '/Users/alliblk/Desktop/gitrepos/zika-usvi/data/fastas/american-zika-{}.fasta'.format(date)\n",
    "americas_frenchpol_file = '/Users/alliblk/Desktop/gitrepos/zika-usvi/data/fastas/american-frenchPolyn-zika-{}.fasta'.format(date)\n",
    "\n",
    "usvi_file = \"/Users/alliblk/Desktop/gitrepos/zika-usvi/data/fastas/usvi-{}.fasta\".format(date)\n",
    "usvi_primary_clade_file = '/Users/alliblk/Desktop/gitrepos/zika-usvi/data/fastas/usvi-primary-clade-{}.fasta'.format(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sequences that should be removed from the analysis alignment #\n",
    "# (reasons for removal are noted in the comments) #\n",
    "\n",
    "#### geographic exclusion criteria #### \n",
    "regions_to_exclude1 = ['southeast_asia', 'oceania', 'japan_korea', 'china','europe'] #french polynesia out\n",
    "regions_to_exclude2 = ['southeast_asia', 'japan_korea', 'china','europe'] #french polynesia in\n",
    "\n",
    "#### sequence characteristic exclusion criteria (based on Augur processing) #### \n",
    "drop_for_indel = [\"CX17\"] #sequence has large number of indels.\n",
    "drop_for_contamination = ['ZF36_36S'] #possible contamination.\n",
    "drop_duplicates = [\"Dominican_Republic/2016/PD2\", \"GD01\", \"GDZ16001\", \"VEN/UF_2/2016\"] #true strains, but duplicates of other strains\n",
    "#excessive terminal branch length, likely indicative of large amount of sequencing error.\n",
    "drop_for_excessive_terminal_branch_length = [\"Bahia04\", \"JAM/2016/WI_JM6\", \"Bahia11\", \"Bahia12\", \"DOM/2016/MA_WGS16_009\", \"VE_Ganxian\", \"BRA/2016/FC_DQ60D1\", \"CX5\"]\n",
    "drop_unknown_export = [\"VR10599/Pavia/2016\", \"34997/Pavia/2016\"] # travel cases where country of infection acquisition is unknown\n",
    "drop_molecClock_off = [\"THA/PLCal_ZV/2013\", \"SK403/13AS\", \"SV0010/15\", \"SK364/13AS\"] #outliers on root to tip analyses. Aren't adhering to molecular clock.\n",
    "drop_augurPruned = ['Bahia05', 'Bahia15'] #An augur run will spit this out as verbose output, and you can also tell based on what sequences are included in the JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 501 Zika sequences available from fauna\n",
      "There are 335 American Zika sequences available from fauna\n",
      "Augur removed 1 sequence(s) sampled from the Americas because they contain excessive indels\n",
      "Augur removed 0 sequence(s) sampled from the Americas because they show evidence of contamination\n",
      "Augur removed 3 sequence(s) sampled from the Americas because they are duplicate strains\n",
      "Augur removed 8 sequence(s) sampled from the Americas because they demonstrate excessive terminal branch length\n",
      "Augur removed 0 sequence(s) sampled from the Americas because they are exported cases where country of infection acquistion could not be determined\n",
      "Augur removed 0 sequence(s) sampled from the Americas because they did not follow the molecular clock\n",
      "Alli removed 2 additional sequence(s) because they outliers/poor quality/generally problematic\n"
     ]
    }
   ],
   "source": [
    "# Get stats on how many of the Americas fauna sequences are dropped by augur because they are problematic.\n",
    "\n",
    "seqs_in_americas_count = 0\n",
    "indel_drop_count = 0\n",
    "contam_drop_count = 0\n",
    "duplicate_drop_count = 0\n",
    "term_branch_leng_drop_count = 0\n",
    "export_drop_count = 0\n",
    "clock_drop_count = 0\n",
    "augur_pruned_count = 0 \n",
    "\n",
    "for key in strain_header_dict.keys():\n",
    "    if strain_header_dict[key].split('|')[4] in regions_to_exclude1: #only look at Americas sequences\n",
    "        continue\n",
    "    else:\n",
    "        seqs_in_americas_count += 1\n",
    "        if key in drop_for_indel:\n",
    "            indel_drop_count += 1\n",
    "        elif key in drop_for_contamination:\n",
    "            contam_drop_count += 1\n",
    "        elif key in drop_duplicates:\n",
    "            duplicate_drop_count += 1\n",
    "        elif key in drop_for_excessive_terminal_branch_length:\n",
    "            term_branch_leng_drop_count += 1\n",
    "        elif key in drop_unknown_export:\n",
    "            export_drop_count += 1\n",
    "        elif key in drop_molecClock_off:\n",
    "            clock_drop_count += 1\n",
    "        elif key in drop_augurPruned:\n",
    "            augur_pruned_count += 1\n",
    "\n",
    "print \"There are {} Zika sequences available from fauna\".format(len(strain_header_dict.keys()))\n",
    "print \"There are {} American Zika sequences available from fauna\".format(seqs_in_americas_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they contain excessive indels\".format(indel_drop_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they show evidence of contamination\".format(contam_drop_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they are duplicate strains\".format(duplicate_drop_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they demonstrate excessive terminal branch length\".format(term_branch_leng_drop_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they are exported cases where country of infection acquistion could not be determined\".format(export_drop_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they did not follow the molecular clock\".format(clock_drop_count)\n",
    "print \"Alli removed {} additional sequence(s) because they outliers/poor quality/generally problematic\".format(augur_pruned_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# make dicts holding data in different structures\n",
    "\n",
    "# dict that will allow matching of fauna header to MSA header\n",
    "with open(fauna_file,'rU') as file:\n",
    "    strain_header_dict={line.split('|')[0].replace('>',''):line.strip() for line in file if line.startswith('>')}\n",
    "\n",
    "# dict that will allow matching of fauna header to fauna sequnces (need this for making augur-formatted infiles)\n",
    "fauna_dict = SeqIO.to_dict(SeqIO.parse(fauna_file, 'fasta'))\n",
    "print len(fauna_dict)\n",
    "\n",
    "# dict that has only strain name as key, but value is aligned, stripped to reference sequence\n",
    "zika_msa = AlignIO.read(open(zika_msa_stripped),'fasta')\n",
    "zika_msa_dict = {record.id:record.seq for record in zika_msa}\n",
    "\n",
    "#samples that Augur prunes during tree building, therefore the genomes are still in the MSA and need to be removed\n",
    "zika_msa_dict_pruned = {key:value for key,value in zika_msa_dict.items() if key not in drop_augurPruned}\n",
    "print len(zika_msa_dict_pruned)\n",
    "\n",
    "### NOTE TO SELF:\n",
    "#now that I'm doing all the trouble shooting in Augur, the sequence set that I want will probably be \n",
    "#the input to Augur, IE I probably won't need a pruned MSA dict in the future.\n",
    "\n",
    "\n",
    "#check that you in fact have the trimmed down alignment loaded in.\n",
    "for key in zika_msa_dict_pruned.keys():\n",
    "    assert len(zika_msa_dict_pruned[key]) == 10769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bahia09|zika|KU940224|2015-08-01|south_america|brazil|brazil|brazil|genbank|genome|Naccache', 'Bahia04|zika|KX101062|2015-06-XX|south_america|brazil|brazil|brazil|genbank|genome|Naccache', 'Bahia07|zika|KU940228|2015-07-01|south_america|brazil|brazil|brazil|genbank|genome|Naccache', 'Bahia15|zika|KX101065|2016-01-XX|south_america|brazil|brazil|brazil|genbank|genome|Naccache', 'Bahia12|zika|KX101067|2015-05-XX|south_america|brazil|brazil|brazil|genbank|genome|Naccache', 'Bahia02|zika|KX101060|2015-05-XX|south_america|brazil|brazil|brazil|genbank|genome|Naccache', 'Bahia01|zika|KX101066|2015-05-XX|south_america|brazil|brazil|brazil|genbank|genome|Naccache', 'Bahia03|zika|KX101061|2015-05-XX|south_america|brazil|brazil|brazil|genbank|genome|Naccache', 'Bahia08|zika|KU940227|2015-07-15|south_america|brazil|brazil|brazil|genbank|genome|Naccache', 'Bahia11|zika|KX101064|2015-04-XX|south_america|brazil|brazil|brazil|genbank|genome|Naccache', 'Bahia05|zika|KX101063|2015-12-XX|south_america|brazil|brazil|brazil|genbank|genome|Naccache']\n",
      "{'Bahia11': 3907, 'Bahia04': 5837, 'Bahia05': 5876, 'Bahia15': 5501, 'Bahia07': 0, 'Bahia12': 5190, 'Bahia02': 2407, 'Bahia03': 2274, 'Bahia08': 1702, 'Bahia09': 2, 'Bahia01': 2790}\n",
      "{'Bahia11': 6828, 'Bahia04': 4486, 'Bahia05': 4564, 'Bahia15': 4847, 'Bahia07': 10676, 'Bahia12': 5376, 'Bahia02': 7924, 'Bahia03': 8290, 'Bahia08': 8849, 'Bahia09': 10523, 'Bahia01': 7842}\n"
     ]
    }
   ],
   "source": [
    "#trouble shooting some samples that are slightly poorer quality. Trying to see what propotion of the genome is N \n",
    "# for some samples that are looking a bit odd in the tree.\n",
    "naccache_dict = {key:value for key,value in fauna_dict.items() if key.startswith('Bahia')}\n",
    "print naccache_dict.keys()\n",
    "n_counts = {}\n",
    "non_n_counts = {}\n",
    "\n",
    "for key in naccache_dict.keys():\n",
    "    n_count = 0\n",
    "    seq_len = len(naccache_dict[key].seq)\n",
    "    for base in naccache_dict[key].seq:\n",
    "        if base == 'n':\n",
    "            n_count +=1\n",
    "        else:\n",
    "            continue\n",
    "    n_counts[key.split('|')[0]] = n_count\n",
    "    non_n_counts[key.split('|')[0]] = seq_len-n_count\n",
    "    #n_counts[key] = n_count\n",
    "print n_counts\n",
    "print non_n_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making fasta files formatted for input into Augur pipeline.\n",
    "\n",
    "Since Augur is a faster tool for building trees and doing ancestral state reconstruction than BEAST, I'm troubleshooting possible issues with the input alignment and looking for outliers (general dataset QC) via Augur builds. Augur input fasta files need to be formatted exactly the same was as the fauna output fasta file. So what I'm doing here is keeping header formatting the same as fauna, but subsampling down to exclude genomes from countries that I don't want to include, or samples that appear to be outliers etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prune dicts geographically\n",
    "# note that all of the samples that should be dropped except those specified in drop_augurPruned are \n",
    "# hardcoded into Augur as samples that should be dropped.\n",
    "# therefore they do not need to be dropped here in this script\n",
    "# I've entered them above mainly so I can keep stats on numbers of samples getting dropped and why\n",
    "geoPruned_fauna_dict_americasOnly = {key:value for key,value in fauna_dict.items() if key.split('|')[4] not in regions_to_exclude1 and key.split('|')[0] not in drop_augurPruned}\n",
    "geoPruned_fauna_dict_includeOceania = {key:value for key,value in fauna_dict.items() if key.split('|')[4] not in regions_to_exclude2 and key.split('|')[0] not in drop_augurPruned}\n",
    "\n",
    "#print out fauna-format files for input into augur\n",
    "\n",
    "# Americas only\n",
    "with open('/Users/alliblk/Desktop/gitrepos/zika-usvi/data/fastas/augur-americas-only.fasta','w') as file:\n",
    "    for key in geoPruned_fauna_dict_americasOnly.keys():\n",
    "        file.write(str('>' + geoPruned_fauna_dict_americasOnly[key].description + '\\n' + geoPruned_fauna_dict_americasOnly[key].seq + '\\n'))\n",
    "        \n",
    "#Americas + french polynesia\n",
    "with open('/Users/alliblk/Desktop/gitrepos/zika-usvi/data/fastas/augur-americas-andfp.fasta','w') as file:\n",
    "    for key in geoPruned_fauna_dict_includeOceania.keys():\n",
    "        file.write(str('>' + geoPruned_fauna_dict_includeOceania[key].description + '\\n' + geoPruned_fauna_dict_includeOceania[key].seq + '\\n'))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the augur processed multiple sequence alignment with the fauna-output fasta\n",
    "\n",
    "Here I want to combine attributes of both the `Nextstrain/augur` processed Zika MSA with the fasta output from `Nextstrain/fauna`. The Fauna download has the strain information in the desired fasta format, with all necessary metadata (sampling date, geography) in the header. The processed multiple sequence alignment however has been aligned with mafft and stripped to the WHO ZIKV reference genome, and therefore represents the sequence alignment that we want.\n",
    "\n",
    "The header from the MSA contains the strain name of the sample, which is also in the fauna header. Therefore I will use key matching to make a new fasta file that combines the header from the fauna file with the sequences from the augur msa. The fauna header will be trimmed down when writing out the fastas to remove superfluous information in the header and to ensure that the headers can be read in to FigTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325\n",
      "299\n",
      "26\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "output_dict = {}\n",
    "for key in zika_msa_dict_pruned.keys():\n",
    "    header = strain_header_dict[key]\n",
    "    seq = zika_msa_dict_pruned[key]\n",
    "    output_dict[header] = seq\n",
    "\n",
    "americas_count = 0\n",
    "oceania_count = 0\n",
    "non_americas_non_oceania_count = 0 \n",
    "\n",
    "for key in output_dict.keys():\n",
    "    if key.split('|')[4] not in regions_to_exclude1:\n",
    "        americas_count += 1\n",
    "    elif key.split('|')[4] == 'oceania':\n",
    "        oceania_count += 1\n",
    "    elif key.split('|')[4] in regions_to_exclude1:\n",
    "        non_americas_non_oceania_count += 1\n",
    "\n",
    "print len(output_dict.keys())\n",
    "print americas_count\n",
    "print oceania_count\n",
    "print non_americas_non_oceania_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print out Americas only multiple sequence alignment\n",
    "with open(americas_file,'w') as out_file:\n",
    "    for key in output_dict.keys():\n",
    "        if key.split('|')[4] in regions_to_exclude1:\n",
    "            continue\n",
    "        else:\n",
    "            split_name = key.split('|')\n",
    "            header = split_name[0] +'|'+ split_name[3] + '|'+ split_name[4] + '|'+ split_name[5]\n",
    "            out_file.write(str(header + '\\n' + output_dict[key] + '\\n'))\n",
    "\n",
    "# Uncomment to print out alignment that still contains french polynesian sequences\n",
    "#print out Americas and french polynesia multiple sequence alignment\n",
    "'''\n",
    "with open(americas_frenchpol_file,'w') as out_file:\n",
    "    for key in output_dict.keys():\n",
    "        if key.split('|')[4] in regions_to_exclude2:\n",
    "            continue\n",
    "        else:\n",
    "            split_name = key.split('|')\n",
    "            header = split_name[0] +'|'+ split_name[3] + '|'+ split_name[4] + '|'+ split_name[5] \n",
    "            out_file.write(str(header + '\\n' + output_dict[key] + '\\n'))\n",
    "'''\n",
    "#print out USVI only multiple sequence alignment\n",
    "with open(usvi_file,'w') as out_file:\n",
    "    for key in output_dict.keys():\n",
    "        if key.split('|')[5] == 'usvi':\n",
    "            split_name = key.split('|')\n",
    "            header = split_name[0] +'|'+ split_name[3] + '|'+ split_name[4] + '|'+ split_name[5]\n",
    "            out_file.write(str(header + '\\n' + output_dict[key] + '\\n'))\n",
    "        else:\n",
    "            continue\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

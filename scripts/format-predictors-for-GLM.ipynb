{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_countries = ['united_states','canada','mexico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once linearized, the 1D predictor matrix should be length_matrix times length_matrix - 1 long. This is because the diagonal values (eg canada to canada) are removed from the linearized matrix before it is put in the xml. Thus, if you have 45 countries in your GLM, then the linearized matrix should be 45 * 44 entries long. \n",
    "\n",
    "Note that indexing for GLM matrices in BEAST is also a little different. All entries to the top of the diagonal are sequentially filled in first, row by row. Then cell entries on the bottom of the matrix are filled in, row by row.\n",
    "\n",
    "XXXXXXX| usa |canada| mexico\n",
    "-------|-----|-------|-------\n",
    "usa    |XXXXX|  0    | 1\n",
    "canada |3    |XXXXXXX| 2\n",
    "mexico |4    |  5    | XXXXXX\n",
    "\n",
    "When linearized for the xml, the above predictor matrix takes the form of `[0, 1, 2, 3, 4, 5]`.\n",
    "\n",
    "If we have a dictionary, where the index is the key, and the value is a tuple, in form `(origin, destination)`, then the dictionary should look like this:\n",
    "\n",
    "`{0: ('usa', 'canada'), 1: ('usa', 'mexico'), 2: ('canada', 'mexico'), 3: ('canada', 'usa'), 4: ('mexico', 'usa'), 5: ('mexico', 'canada')}`\n",
    "\n",
    "Below, I'm formatting things a little differently. I'm actually going to use a nested dictionary structure where the key is the (origin,destination) tuple, and the value is a dictionary that includes index value, and  predictor values as pulled from a tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify functions\n",
    "\n",
    "def logMatrix(matrix):\n",
    "    transformed_matrix = [np.log(value) for value in matrix]\n",
    "    return transformed_matrix\n",
    "\n",
    "def standardizeMatrix(matrix):\n",
    "    mean = np.nanmean(matrix)\n",
    "    stdev = np.nanstd(matrix)\n",
    "    standardized_matrix = [(value - mean)/stdev for value in matrix if value != float('nan')]\n",
    "    return standardized_matrix\n",
    "\n",
    "def make_GLMmatrix(predictor_dict,predictor_name_string):\n",
    "    linearized_predictor = []\n",
    "    for i in range(len(predictor_dict)):\n",
    "        try: \n",
    "            linearized_predictor.append(predictor_dict[i][predictor_name_string])\n",
    "        except KeyError:\n",
    "            linearized_predictor.append(float('nan'))\n",
    "    logged_matrix = logMatrix(linearized_predictor)\n",
    "    std_log_matrix = standardizeMatrix(logged_matrix)\n",
    "    return std_log_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initially try everything out on a test dataset.\n",
    "\n",
    "I've made a little test tsv file, and have a small list of three countries that I want to set up predictors for. Just making sure here that everything is working the way I think it should on a dataset that's small enough that I can spot errors easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code to make the indexing dictionaries, adapted from Gytis Dudas' EBOV iPython notebook.\n",
    "# nothing crazy, just annoying math to deal with the bizarre GLM indexing.\n",
    "\n",
    "test_predictor_dict={}\n",
    "matrix_length=len(test_countries)\n",
    "for i in range(len(test_countries)):\n",
    "    for j in range(i+1,len(test_countries)): #make the second iteration 1 shorter than the first\n",
    "        index_1=int((matrix_length*(matrix_length-1)/2) - (matrix_length-i)*((matrix_length-i)-1)/2 + j - i - 1)\n",
    "        index_2=int((matrix_length*(matrix_length-1)) - (matrix_length-i)*((matrix_length-i)-1)/2 + j - i - 1)\n",
    "\n",
    "        test_predictor_dict[index_1] = {'country_pair':(test_countries[i],test_countries[j])}\n",
    "        test_predictor_dict[index_2] = {'country_pair':(test_countries[j],test_countries[i])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try importing test predictor set, and assigning great circle distances\n",
    "\n",
    "test_infile = '/Users/alliblk/Desktop/gitrepos/zika-usvi/data/predictors/predictor-tsv/test-predictors.tsv'\n",
    "with open(test_infile,'rU') as file:\n",
    "    for line in file:\n",
    "        if line.startswith('origin'): # this line is the header of the tsv\n",
    "            predictor  = line.strip().split('\\t')[2] #predictor name in the tsv is what it will be called in dict\n",
    "        else:\n",
    "            country_tuple = (line.split('\\t')[0],line.split('\\t')[1]) # origin,destination tuple\n",
    "            for key in test_predictor_dict.keys():\n",
    "                if test_predictor_dict[key]['country_pair'] == country_tuple:\n",
    "                    test_predictor_dict[key][predictor] = float(line.strip().split('\\t')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'country_pair': ('united_states', 'canada'), 'great_circle_dist_km': 1077.10699075}, 1: {'country_pair': ('united_states', 'mexico'), 'great_circle_dist_km': 1922.06469663}, 2: {'country_pair': ('canada', 'mexico'), 'great_circle_dist_km': 2997.40071088}, 3: {'country_pair': ('canada', 'united_states'), 'great_circle_dist_km': 1077.10699075}, 4: {'country_pair': ('mexico', 'united_states'), 'great_circle_dist_km': 1922.06469663}, 5: {'country_pair': ('mexico', 'canada'), 'great_circle_dist_km': 2997.40071088}}\n"
     ]
    }
   ],
   "source": [
    "print test_predictor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('united_states', 'canada')\n",
      "('united_states', 'mexico')\n",
      "('canada', 'mexico')\n",
      "('canada', 'united_states')\n",
      "('mexico', 'united_states')\n",
      "('mexico', 'canada')\n",
      "[1077.10699075, 1922.06469663, 2997.40071088, 1077.10699075, 1922.06469663, 2997.40071088]\n"
     ]
    }
   ],
   "source": [
    "# CHECK TO MAKE SURE EVERYTHING IS WORKING RIGHT!!! Should match up with indexing of markdown table above.\n",
    "\n",
    "test_linearized_predictor = []\n",
    "\n",
    "for i in range(len(test_predictor_dict)):\n",
    "    print test_predictor_dict[i]['country_pair']\n",
    "    test_linearized_predictor.append(test_predictor_dict[i]['great_circle_dist_km'])\n",
    "\n",
    "print test_linearized_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.9820340136801056, 7.5611552500615309, 8.0055007623764105, 6.9820340136801056, 7.5611552500615309, 8.0055007623764105]\n",
      "7.51623000871\n",
      "0.41903440941\n"
     ]
    }
   ],
   "source": [
    "#try out log transforming matrix\n",
    "\n",
    "testLog_predictor = logMatrix(test_linearized_predictor)\n",
    "print testLog_predictor #yes, this looks right\n",
    "print np.mean(testLog_predictor)\n",
    "print np.std(testLog_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2748260835608844, 0.10721134194857139, 1.1676147416123193, -1.2748260835608844, 0.10721134194857139, 1.1676147416123193]\n"
     ]
    }
   ],
   "source": [
    "std_matrix = standardizeMatrix(testLog_predictor)\n",
    "print std_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2748260835608844, 0.10721134194857139, 1.1676147416123193, -1.2748260835608844, 0.10721134194857139, 1.1676147416123193]\n"
     ]
    }
   ],
   "source": [
    "#test that function that includes logging and standardizing does the same thing:\n",
    "func_test_matrix = make_GLMmatrix(test_predictor_dict,'great_circle_dist_km')\n",
    "print func_test_matrix\n",
    "\n",
    "for i in xrange(len(test_countries)):\n",
    "    if std_matrix[i] != func_test_matrix[i]:\n",
    "        print i\n",
    "        print 'Houston we have a problem'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing looks good. Now we'll do it for the full set of 45 countries to include in the GLM. \n",
    "\n",
    "1) Import from indexed-countries-45.tsv to get the full list of countries that will be used.\n",
    "2) Start by importing all countryXcountry great circle distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['canada', 'united_states', 'bermuda', 'mexico', 'belize', 'guatemala', 'honduras', 'el_salvador', 'nicaragua', 'costa_rica', 'panama', 'bahamas', 'cuba', 'turks_caicos_islands', 'cayman_islands', 'jamaica', 'haiti', 'dominican_republic', 'puerto_rico', 'united_states_virgin_islands', 'saint_kitts_nevis', 'antigua_barbuda', 'guadeloupe', 'dominica', 'martinique', 'saint_lucia', 'saint_vincent_grenadines', 'barbados', 'grenada', 'trinidad_tobago', 'curacao', 'aruba', 'french_guiana', 'suriname', 'guyana', 'venezuela', 'colombia', 'ecuador', 'peru', 'bolivia', 'brazil', 'paraguay', 'uruguay', 'argentina', 'chile']\n",
      "\n",
      " 45 countries included in analysis\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/alliblk/Desktop/gitrepos/zika-usvi/data/indexed-countries-45.tsv') as file:\n",
    "    countries_list = [line.strip().split('\\t')[0] for line in file if not line.startswith('country')]\n",
    "    \n",
    "print countries_list #this should be ordered north to south\n",
    "\n",
    "print '\\n {} countries included in analysis'.format(len(countries_list)) #this should be 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make predictor dictionary. Index is key, value is dict with origin,destination tuple.\n",
    "\n",
    "predictor_dict = {}\n",
    "\n",
    "matrix_length=len(countries_list)\n",
    "\n",
    "for i in range(len(countries_list)):\n",
    "    for j in range(i+1,len(countries_list)): #make the second iteration 1 shorter than the first\n",
    "        index_1=int((matrix_length*(matrix_length-1)/2) - (matrix_length-i)*((matrix_length-i)-1)/2 + j - i - 1)\n",
    "        index_2=int((matrix_length*(matrix_length-1)) - (matrix_length-i)*((matrix_length-i)-1)/2 + j - i - 1)\n",
    "\n",
    "        predictor_dict[index_1] = {'country_pair':(countries_list[i],countries_list[j])}\n",
    "        predictor_dict[index_2] = {'country_pair':(countries_list[j],countries_list[i])}\n",
    "\n",
    "assert len(predictor_dict) == len(countries_list)**2 - len(countries_list), 'predictor dictionary malformed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a reference, I'm going to write the index for each country pair to a tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Users/alliblk/Desktop/gitrepos/zika-usvi/data/index-countrypair-mapping.tsv','w') as file:\n",
    "    file.write('{}\\t{}\\n'.format('index', 'country_pair'))\n",
    "    for i in range(len(predictor_dict)):\n",
    "        file.write('{}\\t{}\\n'.format(i, predictor_dict[i]['country_pair']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add predictor values to the dictionary based on key matching of the country pair tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing predictor from /Users/alliblk/Desktop/gitrepos/zika-usvi/data/predictors/predictor-tsv/destination-popsize.tsv\n",
      "importing predictor from /Users/alliblk/Desktop/gitrepos/zika-usvi/data/predictors/predictor-tsv/great-circle-dists.tsv\n",
      "importing predictor from /Users/alliblk/Desktop/gitrepos/zika-usvi/data/predictors/predictor-tsv/north-south-indicator.tsv\n",
      "importing predictor from /Users/alliblk/Desktop/gitrepos/zika-usvi/data/predictors/predictor-tsv/origin-popsize.tsv\n",
      "importing predictor from /Users/alliblk/Desktop/gitrepos/zika-usvi/data/predictors/predictor-tsv/pax-volume-snakecase.tsv\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/alliblk/Desktop/gitrepos/zika-usvi/data/predictors/predictor-tsv/*.tsv\"\n",
    "\n",
    "for fname in glob.glob(path):\n",
    "    #skip testing tsv\n",
    "    if fname == '/Users/alliblk/Desktop/gitrepos/zika-usvi/data/predictors/predictor-tsv/test-predictors.tsv':\n",
    "        continue\n",
    "    else:\n",
    "        with open(fname,'rU') as file:\n",
    "            print 'importing predictor from {}'.format(fname)\n",
    "            for line in file:\n",
    "                if line.startswith('origin'): # this line is the header of the tsv\n",
    "                    predictor  = line.strip().split('\\t')[2] #predictor name in the tsv is what it will be called in dict\n",
    "                else:\n",
    "                    country_tuple = (line.split('\\t')[0],line.split('\\t')[1]) # origin,destination tuple\n",
    "                    for key in predictor_dict.keys(): #iterate through the indices\n",
    "                        if predictor_dict[key]['country_pair'] == country_tuple:\n",
    "                            predictor_dict[key][predictor] = float(line.strip().split('\\t')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do some manual checking to make sure that import went smoothly. Print output and look up values in tsv.\n",
    "\n",
    "##### Note to self:\n",
    "For the passenger volume data, if there was no volume between two locations, there was no entry in the tsv file. Just need to be careful then that when we write out the matrices we state `NaN` if `pax_volume` does not exist for a country pair. I'm going to make a tsv with these country pairs and their indices to keep track of them more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980\n",
      "\n",
      "\n",
      "{'north_south_indicator': 1.0, 'destination_pop_size': 323995528.0, 'country_pair': ('canada', 'united_states'), 'great_circle_dist_km': 1077.10699075, 'origin_pop_size': 35362905.0, 'pax_volume': 12361348.0}\n",
      "\n",
      "\n",
      "{'north_south_indicator': 1.0, 'destination_pop_size': 52329.0, 'country_pair': ('turks_caicos_islands', 'saint_kitts_nevis'), 'great_circle_dist_km': 1884.07804402, 'origin_pop_size': 51430.0, 'pax_volume': 45.0}\n",
      "\n",
      "\n",
      "{'north_south_indicator': -1.0, 'destination_pop_size': 6156670.0, 'country_pair': ('aruba', 'el_salvador'), 'great_circle_dist_km': 2075.47088424, 'origin_pop_size': 113648.0, 'pax_volume': 125.0}\n",
      "\n",
      "\n",
      "{'destination_pop_size': 252338.0, 'country_pair': ('honduras', 'french_guiana'), 'great_circle_dist_km': 3939.8419472, 'north_south_indicator': 1.0, 'origin_pop_size': 8893259.0}\n",
      "\n",
      "\n",
      "{'destination_pop_size': 52329.0, 'country_pair': ('paraguay', 'saint_kitts_nevis'), 'great_circle_dist_km': 4749.19993604, 'north_south_indicator': -1.0, 'origin_pop_size': 6862812.0}\n"
     ]
    }
   ],
   "source": [
    "#Check a few of the entries manually to make sure everything worked.\n",
    "print len(predictor_dict)\n",
    "print '\\n'\n",
    "print predictor_dict[0]\n",
    "print '\\n'\n",
    "print predictor_dict[500]\n",
    "print '\\n'\n",
    "print predictor_dict[1300]\n",
    "print '\\n'\n",
    "print predictor_dict[274]\n",
    "print '\\n'\n",
    "\n",
    "#example of entry with no pax_volume between the origin and the destination. \n",
    "#use KeyError as condition.\n",
    "print predictor_dict[1700]['pax_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make note of which country pairs do not have any travel between them, and their indices!\n",
    "\n",
    "with open('/Users/alliblk/Desktop/gitrepos/zika-usvi/data/predictors/countries-without-pax-volume.tsv','w') as file:\n",
    "    file.write('{}\\t{}\\n'.format('index', 'country_pair_without_pax_volume'))\n",
    "    for i in range(len(predictor_dict)):\n",
    "        try:\n",
    "            predictor_dict[i]['pax_volume']\n",
    "        except KeyError:\n",
    "            file.write('{}\\t{}\\n'.format(i, predictor_dict[i]['country_pair']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary looks good, and values in dict match what I look up manually in the tsv files. Now to export the matrices for each predictor, log transform, and standardize.\n",
    "\n",
    "For Tuesday: need to figure out what is breaking north_south_matrix transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.84505511502939379, 0.21548402909924946, 0.40775586743969749, 0.50634161354634444, 0.61696738620070901, 0.61225131986801407, 0.65474130853847223, 0.70567340993535066, 0.79202988503799654, 0.84348082810185299, 0.21146112772662665, 0.35049625982852151, 0.47202716859598348, 0.45297175878924129, 0.54250357800709514, 0.57000157707956056, 0.60092770337070589, 0.68039655791555653, 0.70651915037242341, 0.90884992525427299, 0.78882317049990314, 0.81692298793805507, 0.84448791753816477, 0.87052455974249499, 0.89002911123151385, 0.9068348508582782, 0.93091709993708793, 0.92711936538944462, 0.97395261058058924, 0.8360548868841009, 0.81290401269595935, 1.2046086988231124, 1.1588830285499836, 1.1095597845519656, 0.90908626070686904, 0.96031577171400606, 1.1357015125660033, 1.3591318743202783, 1.5281629502527887, 1.6643479088215185]\n"
     ]
    }
   ],
   "source": [
    "gcd_transformed = make_GLMmatrix(predictor_dict,'great_circle_dist_km')\n",
    "originPopSz_transformed = make_GLMmatrix(predictor_dict,'origin_pop_size')\n",
    "destinPopSz_transformed = make_GLMmatrix(predictor_dict,'destination_pop_size')\n",
    "pax_volume_transformed = make_GLMmatrix(predictor_dict, 'pax_volume')\n",
    "\n",
    "with open('/Users/alliblk/Desktop/gitrepos/zika-usvi/data/predictors/transformed-linearized-matrices/','w'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:4: RuntimeWarning: invalid value encountered in log\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:10: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "#north_south_transformed = make_GLMmatrix(predictor_dict,'north_south_indicator')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

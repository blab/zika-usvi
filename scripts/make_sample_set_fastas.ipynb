{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### import libraries ####\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "import datetime\n",
    "\n",
    "date = datetime.datetime.now().strftime (\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### infile paths #### \n",
    "zika_msa_stripped = \"/Users/alliblk/Desktop/gitrepos/augur/zika/processed/zika_aligned_stripped.mfa\"\n",
    "fauna_file = \"/Users/alliblk/Desktop/gitrepos/fauna/data/zika.fasta\"\n",
    "\n",
    "#### outfile paths #### \n",
    "americas_file = '/Users/alliblk/Desktop/gitrepos/zika-usvi/data/fastas/american-zika-{}.fasta'.format(date)\n",
    "americas_frenchpol_file = '/Users/alliblk/Desktop/gitrepos/zika-usvi/data/fastas/american-frenchPolyn-zika-{}.fasta'.format(date)\n",
    "\n",
    "usvi_file = \"/Users/alliblk/Desktop/gitrepos/zika-usvi/data/fastas/usvi-{}.fasta\".format(date)\n",
    "usvi_primary_clade_file = '/Users/alliblk/Desktop/gitrepos/zika-usvi/data/fastas/usvi-primary-clade-{}.fasta'.format(date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the augur processed multiple sequence alignment with the fauna-output fasta\n",
    "\n",
    "Here I want to combine attributes of both the `Nextstrain/augur` processed Zika MSA with the fasta output from `Nextstrain/fauna`. The Fauna download has the strain information in the desired fasta format, with all necessary metadata (sampling date, geography) in the header. The processed multiple sequence alignment however has been aligned with mafft and stripped to the WHO ZIKV reference genome, and therefore represents the sequence the alignment that I want.\n",
    "\n",
    "The header from the MSA contains the strain name of the sample, which is also in the fauna header. Therefore I will use key matching to make a new fasta file that combines the header from the fauna file with the sequences from the augur msa.\n",
    "\n",
    "Note to self: Might not be a bad idea to have this capability in Augur, should potentially open an issue to prompt discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sequences that should be removed from the analysis alignment #\n",
    "# (reasons for removal are noted in the comments) #\n",
    "\n",
    "#### geographic exclusion criteria #### \n",
    "regions_to_exclude1 = ['southeast_asia', 'oceania', 'japan_korea', 'china','europe'] #french polynesia out\n",
    "regions_to_exclude2 = ['southeast_asia', 'japan_korea', 'china','europe'] #french polynesia in\n",
    "\n",
    "#### sequence characteristic exclusion criteria (based on Augur processing) #### \n",
    "drop_for_indel = [\"CX17\"] #sequence has large number of indels.\n",
    "drop_for_contamination = ['ZF36_36S'] #possible contamination.\n",
    "drop_duplicates = [\"Dominican_Republic/2016/PD2\", \"GD01\", \"GDZ16001\", \"VEN/UF_2/2016\"] #true strains, but duplicates of other strains\n",
    "#excessive terminal branch length, likely indicative of large amount of sequencing error.\n",
    "drop_for_excessive_terminal_branch_length = [\"Bahia04\", \"JAM/2016/WI_JM6\", \"Bahia11\", \"Bahia12\", \"DOM/2016/MA_WGS16_009\", \"VE_Ganxian\", \"BRA/2016/FC_DQ60D1\", \"CX5\"]\n",
    "drop_unknown_export = [\"VR10599/Pavia/2016\", \"34997/Pavia/2016\"] # travel cases where country of infection acquisition is unknown\n",
    "drop_molecClock_off = [\"THA/PLCal_ZV/2013\", \"SK403/13AS\", \"SV0010/15\", \"SK364/13AS\"] #outliers on root to tip analyses. Aren't adhering to molecular clock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in headers from fauna output file\n",
    "with open(fauna_file,'rU') as file:\n",
    "    strain_header_dict={line.split('|')[0].replace('>',''):line.strip() for line in file if line.startswith('>')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">DOM/2016/MA_WGS16_020|zika|KY785460|2016-06-30|north_america|dominican_republic|dominican_republic|dominican_republic|genbank|genome|Metsky et al|https://www.ncbi.nlm.nih.gov/nuccore/KY785460\n"
     ]
    }
   ],
   "source": [
    "print strain_header_dict['DOM/2016/MA_WGS16_020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 501 Zika sequences available from fauna\n",
      "There are 335 American Zika sequences available from fauna\n",
      "Augur removed 1 sequence(s) sampled from the Americas because they contain excessive indels\n",
      "Augur removed 0 sequence(s) sampled from the Americas because they show evidence of contamination\n",
      "Augur removed 3 sequence(s) sampled from the Americas because they are duplicate strains\n",
      "Augur removed 8 sequence(s) sampled from the Americas because they demonstrate excessive terminal branch length\n",
      "Augur removed 0 sequence(s) sampled from the Americas because they are exported cases where country of infection acquistion could not be determined\n",
      "Augur removed 0 sequence(s) sampled from the Americas because they did not follow the molecular clock\n"
     ]
    }
   ],
   "source": [
    "# Get stats on how many of the Americas fauna sequences are dropped by augur because they are problematic.\n",
    "\n",
    "seqs_in_americas_count = 0\n",
    "indel_drop_count = 0\n",
    "contam_drop_count = 0\n",
    "duplicate_drop_count = 0\n",
    "term_branch_leng_drop_count = 0\n",
    "export_drop_count = 0\n",
    "clock_drop_count = 0\n",
    "\n",
    "for key in strain_header_dict.keys():\n",
    "    if strain_header_dict[key].split('|')[4] in regions_to_exclude1: #only look at Americas sequences\n",
    "        continue\n",
    "    else:\n",
    "        seqs_in_americas_count += 1\n",
    "        if key in drop_for_indel:\n",
    "            indel_drop_count += 1\n",
    "        elif key in drop_for_contamination:\n",
    "            contam_drop_count += 1\n",
    "        elif key in drop_duplicates:\n",
    "            duplicate_drop_count += 1\n",
    "        elif key in drop_for_excessive_terminal_branch_length:\n",
    "            term_branch_leng_drop_count += 1\n",
    "        elif key in drop_unknown_export:\n",
    "            export_drop_count += 1\n",
    "        elif key in drop_molecClock_off:\n",
    "            clock_drop_count += 1\n",
    "\n",
    "print \"There are {} Zika sequences available from fauna\".format(len(strain_header_dict.keys()))\n",
    "print \"There are {} American Zika sequences available from fauna\".format(seqs_in_americas_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they contain excessive indels\".format(indel_drop_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they show evidence of contamination\".format(contam_drop_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they are duplicate strains\".format(duplicate_drop_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they demonstrate excessive terminal branch length\".format(term_branch_leng_drop_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they are exported cases where country of infection acquistion could not be determined\".format(export_drop_count)\n",
    "print \"Augur removed {} sequence(s) sampled from the Americas because they did not follow the molecular clock\".format(clock_drop_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482\n"
     ]
    }
   ],
   "source": [
    "# read in sequences from multiple sequence alignment\n",
    "# currently header in msa is only strain name. Need to grab all metadata as well with key matching against fauna fasta headers.\n",
    "\n",
    "zika_msa = AlignIO.read(open(zika_msa_stripped),'fasta')\n",
    "zika_msa_dict = {record.id:record.seq for record in zika_msa}\n",
    "\n",
    "#check that you in fact have the trimmed down alignment loaded in.\n",
    "for key in zika_msa_dict.keys():\n",
    "    assert len(zika_msa_dict[key]) == 10769\n",
    "\n",
    "print len(zika_msa_dict.keys())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "output_dict = {}\n",
    "for key in zika_msa_dict.keys():\n",
    "    header = strain_header_dict[key]\n",
    "    seq = zika_msa_dict[key]\n",
    "    output_dict[header] = seq\n",
    "\n",
    "americas_count = 0\n",
    "oceania_count = 0\n",
    "non_americas_non_oceania_count = 0 \n",
    "\n",
    "for key in output_dict.keys():\n",
    "    if key.split('|')[4] not in regions_to_exclude1:\n",
    "        americas_count += 1\n",
    "    elif key.split('|')[4] == 'oceania':\n",
    "        oceania_count += 1\n",
    "    elif key.split('|')[4] in regions_to_exclude1:\n",
    "        non_americas_non_oceania_count += 1\n",
    "\n",
    "print len(output_dict.keys())\n",
    "print americas_count\n",
    "print oceania_count\n",
    "print non_americas_non_oceania_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">SG_114|2016-09-08|southeast_asia|singapore\n"
     ]
    }
   ],
   "source": [
    "for key in output_dict.keys():\n",
    "    split_name = key.split('|')\n",
    "    header = split_name[0] +'|'+ split_name[3] + '|'+ split_name[4] + '|'+ split_name[5]\n",
    "    print header\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print out Americas only multiple sequence alignment\n",
    "with open(americas_file,'w') as out_file:\n",
    "    for key in output_dict.keys():\n",
    "        if key.split('|')[4] in regions_to_exclude1:\n",
    "            continue\n",
    "        else:\n",
    "            split_name = key.split('|')\n",
    "            header = split_name[0] +'|'+ split_name[3] + '|'+ split_name[4] + '|'+ split_name[5]\n",
    "            out_file.write(str(header + '\\n' + output_dict[key] + '\\n'))\n",
    "\n",
    "#print out Americas and french polynesia multiple sequence alignment\n",
    "with open(americas_frenchpol_file,'w') as out_file:\n",
    "    for key in output_dict.keys():\n",
    "        if key.split('|')[4] in regions_to_exclude2:\n",
    "            continue\n",
    "        else:\n",
    "            split_name = key.split('|')\n",
    "            header = split_name[0] +'|'+ split_name[3] + '|'+ split_name[4] + '|'+ split_name[5] \n",
    "            out_file.write(str(header + '\\n' + output_dict[key] + '\\n'))\n",
    "\n",
    "#print out USVI only multiple sequence alignment\n",
    "with open(usvi_file,'w') as out_file:\n",
    "    for key in output_dict.keys():\n",
    "        if key.split('|')[5] == 'usvi':\n",
    "            split_name = key.split('|')\n",
    "            header = split_name[0] +'|'+ split_name[3] + '|'+ split_name[4] + '|'+ split_name[5]\n",
    "            out_file.write(str(header + '\\n' + output_dict[key] + '\\n'))\n",
    "        else:\n",
    "            continue\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
